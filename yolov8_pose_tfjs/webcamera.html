<html lang="en">

<head>
  <meta charset="utf-8">
  <title>YOLOv8 TensorflowJs Serving: WebCamera Pose Detection on the Browser</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/tensorflow/4.16.0/tf.js"
    integrity="sha512-h2uoAnR61cun1IpMUeG45l4ZucxXsW265G2ruTZA28qN3SPV2VlE6+34qZYii2JjdxY5vemxH0AHgsw2zoGcNQ=="
    crossorigin="anonymous" referrerpolicy="no-referrer"></script>
  <style>
    #header {
      display: none;
    }

    html,
    body {
      margin: 0;
      padding: 0;
      overflow: hidden;
      width: 100vw;
      height: 100vh;
    }

    .content {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      overflow: hidden;
    }

    .content>video,
    .content>canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
    }

    .credits {
      position: absolute;
      width: 80%;
      margin-left: 10%;
      top: 0;
      height: 10vh;
      left: 10%;
      object-fit: cover;
      z-index: 1000;
      background-color: aliceblue;
      margin-left: auto;
      margin-right: auto;
    }
  </style>
</head>

<body>
  <div id="header">
    <h1>YOLOv8 WebCam Pose Detection Example</h1>
    <p>Serving : <code class="code">yolov8n-pose.onnx</code></p>
    <p>Model size : <code class="code">640x640</code></p>
  </div>
  <div id="root">
    <div class="App">
      <!-- <div class="credits">
        Built by the Distributed Algorithms and Systems and Machine Learning and Computer Vision groups at ISTA,
        Using open-source software: YOLO-V8 Pose Model; TensorFlow.js; template code from https://github.com/akbartus.
      </div> -->
      <div class="credits" style="width: 80vw; height: 8vw; border: 1px solid #ccc; display: flex; align-items: center; justify-content: space-between; padding: 1vw; box-sizing: border-box; font-family: Arial, sans-serif;">
    <!-- Logos Section -->
    <div style="display: flex; gap: 1vw; align-items: center;">
        <div style="width: 6vw; height: 6vw; overflow: hidden; display: flex; align-items: center; justify-content: center; flex-direction: column;">
            <img src="img/ISTA_black.png" alt="Logo 2" style="width: 100%; height: 50%; object-fit: cover; object-position: center;">
            <img src="img/VISTA_black.png" alt="Logo 2" style="width: 100%; height: 22%; object-fit: cover; object-position: center; margin-top: 5px;">
        </div>
        <div style="width: 6vw; height: 6vw; overflow: hidden; display: flex; align-items: center; justify-content: center; background-color: #f0f0f0;">
            <img src="img/DAS_logo.png" alt="Logo 2" style="width: 100%; height: 100%; object-fit: cover; object-position: center;">
        </div>
        <div style="width: 6vw; height: 6vw; overflow: hidden; display: flex; align-items: center; justify-content: center; background-color: #f0f0f0;">
            <img src="img/MLCV_logo_1.png" alt="Logo 3" style="width: 100%; height: 100%; object-fit: cover; object-position: center;">
        </div>
        <div style="width: 6vw; height: 6vw; overflow: hidden; display: flex; align-items: center; justify-content: center; background-color: #f0f0f0;">
            <img src="img/yolo_logo_2.png" alt="Logo 4" style="width: 100%; height: 100%; object-fit: cover; object-position: center;">
        </div>
        <div style="width: 6vw; height: 6vw; overflow: hidden; display: flex; align-items: center; justify-content: center; background-color: #f0f0f0;">
            <img src="img/tf_logo.png" alt="Logo 1" style="width: 100%; height: 100%; object-fit: cover; object-position: center;">
        </div>
    </div>

    <!-- Text Section -->
    <div style="flex: 1; margin-left: 2vw; font-size: 0.9vw; line-height: 1.4;">
        Built by the <strong>Distributed Algorithms and Systems</strong> and <strong>Machine Learning and Computer Vision</strong> groups at ISTA, using open-source software: <strong>YOLO-V8 pose estimation model</strong>; <strong>TensorFlow.js</strong>; template code from <a href="https://github.com/akbartus" target="_blank">https://github.com/akbartus</a>.
    </div>
</div>

      <div class="content">
        <video id="video" autoPlay="true" playsInline="true" muted="true"></video>
        <canvas id="canvas"></canvas>
      </div>
    </div>
  </div>
  <div>
    <script>

      // Add labels if custom
      const labels = [
        "person"
      ];

      // React State implementation in Vanilla JS
      const useState = (defaultValue) => {
        let value = defaultValue;
        const getValue = () => value;
        const setValue = (newValue) => (value = newValue);
        return [getValue, setValue];
      };

      const [loading, setLoading] = useState({ loading: true, progress: 0 });
      const [model, setModel] = useState({
        network: null,
        inputShape: [1, 0, 0, 3],
      });

      const modelName = "yolov8n-pose";
      const modelURL = "model/model.json";

      tf.ready().then(async () => {
        const yolov8 = await tf.loadGraphModel(modelURL, {
          onProgress: (fractions) => {
            setLoading({ loading: true, progress: fractions });
          },
        });
        const dummyInput = tf.ones(yolov8.inputs[0].shape);
        const warmupResults = yolov8.execute(dummyInput);
        setLoading({ loading: false, progress: 1 });
        setModel({
          network: yolov8,
          inputShape: yolov8.inputs[0].shape,
        });
        tf.dispose([warmupResults, dummyInput]);
      });

      // Preprocess image
      function preProcessImage(source, modelWidth, modelHeight) {
        let widthRatio, heightRatio;
        const input = tf.tidy(() => {
          const img = tf.browser.fromPixels(source);
          const [h, w] = img.shape.slice(0, 2);
          const maxSize = Math.max(w, h);
          const imgPadded = img.pad([
            [0, maxSize - h],
            [0, maxSize - w],
            [0, 0],
          ]);
          widthRatio = (maxSize / w) * 1.85;
          heightRatio = maxSize / h;

          return tf.image
            .resizeBilinear(imgPadded, [modelWidth, modelHeight])
            .div(255.0)
            .expandDims(0);
        });
        return [input, widthRatio, heightRatio];
      };

      // Detect 
      async function detect(source, model, canvasRef, callback = () => { }) {
        tf.engine().startScope();
        const [modelWidth, modelHeight] = model.inputShape.slice(1, 3);
        const [input, widthRatio, heightRatio] = preProcessImage(source, modelWidth, modelHeight);
        const predictions = model.network.execute(input);
        const transpose = predictions.transpose([0, 2, 1]);
        const boxes = tf.tidy(() => {
          const w = transpose.slice([0, 0, 2], [-1, -1, 1]);
          const h = transpose.slice([0, 0, 3], [-1, -1, 1]);
          const x1 = tf.sub(transpose.slice([0, 0, 0], [-1, -1, 1]), tf.div(w, 2));
          const y1 = tf.sub(transpose.slice([0, 0, 1], [-1, -1, 1]), tf.div(h, 2));
          return tf.concat(
            [
              y1,
              x1,
              tf.add(y1, h),
              tf.add(x1, w),
            ],
            2
          ).squeeze();
        });

        const scores = tf.tidy(() => {
          const rawScores = transpose.slice([0, 0, 4], [-1, -1, 1]).squeeze();
          return rawScores;
        });
        const landmarks = tf.tidy(() => { return transpose.slice([0, 0, 5], [-1, -1, -1]).squeeze(); });
        const nms = await tf.image.nonMaxSuppressionAsync(boxes, scores, 500, 0.45, 0.3);
        const boxes_data = boxes.gather(nms, 0).dataSync();
        const scores_data = scores.gather(nms, 0).dataSync();
        let landmarks_data = landmarks.gather(nms, 0).dataSync();
        landmarks_data = tf.reshape(landmarks_data, [-1, 3, 17]);
        drawLabels(source, canvasRef, landmarks_data, boxes_data, scores_data, widthRatio, heightRatio);
        tf.dispose([predictions, transpose, boxes, scores, nms]);
        tf.engine().endScope();
      };




      const detectVideo = (vidSource, model, canvasRef) => {

        const detectFrame = async () => {
          detect(vidSource, model, canvasRef, () => {
            requestAnimationFrame(detectFrame); // get another frame
          });
        };

        detectFrame();
      };

      // Colors and landmarks
      const colors = {
        nose: "cyan",
        leftEye: "lime",
        rightEye: "magenta",
        leftEar: "yellow",
        rightEar: "dodgerblue",
        leftShoulder: "hotpink",
        rightShoulder: "orange",
        leftElbow: "turquoise",
        rightElbow: "springgreen",
        leftWrist: "deeppink",
        rightWrist: "chartreuse",
        leftHip: "skyblue",
        rightHip: "coral",
        leftKnee: "mediumspringgreen",
        rightKnee: "deepskyblue",
        leftAnkle: "orangered",
        rightAnkle: "gold"
      };

      const connections = [
        ["nose", "leftEye"],
        ["nose", "rightEye"],
        ["leftEye", "leftEar"],
        ["rightEye", "rightEar"],
        ["leftShoulder", "rightShoulder"],
        ["leftShoulder", "leftElbow"],
        ["rightShoulder", "rightElbow"],
        ["leftElbow", "leftWrist"],
        ["rightElbow", "rightWrist"],
        ["leftShoulder", "leftHip"],
        ["rightShoulder", "rightHip"],
        ["leftHip", "rightHip"],
        ["leftHip", "leftKnee"],
        ["rightHip", "rightKnee"],
        ["leftKnee", "leftAnkle"],
        ["rightKnee", "rightAnkle"]
      ];

      function drawLabels(source, canvasRef, landmarks_data, boxes_data, scores_data, widthRatio, heightRatio) {
        const ctx = canvasRef.getContext("2d");
        ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
        // ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        for (let i = 0; i < scores_data.length; ++i) {
          const score = (scores_data[i] * 100).toFixed(1);
          let [y1, x1, y2, x2] = boxes_data.slice(i * 4, (i + 1) * 4);
          x1 *= widthRatio;
          x2 *= widthRatio;
          y1 *= heightRatio;
          y2 *= heightRatio;
          const width = x2 - x1;
          const height = y2 - y1;
          ctx.fillStyle = colors["nose"];
          ctx.strokeStyle = colors["nose"]
          ctx.lineWidth = Math.max(Math.min(ctx.canvas.width, ctx.canvas.height) / 200, 2.5);
          ctx.strokeRect(x1, y1, width, height);
          let keypoints = landmarks_data.slice([i, 0, 0], [1, -1, -1]).reshape([17, 3]).arraySync();
          ctx.lineWidth = 2;
          ctx.strokeStyle = "white";

          // Draw Connections
          for (const [partA, partB] of connections) {
            const x1 = keypoints[Object.keys(colors).indexOf(partA)][0] * widthRatio;
            const y1 = keypoints[Object.keys(colors).indexOf(partA)][1] * heightRatio;
            const x2 = keypoints[Object.keys(colors).indexOf(partB)][0] * widthRatio;
            const y2 = keypoints[Object.keys(colors).indexOf(partB)][1] * heightRatio;
            ctx.beginPath();
            ctx.moveTo(x1, y1);
            ctx.lineTo(x2, y2);
            ctx.stroke();
            ctx.closePath();
          }
          // Draw Keypoints
          for (let j = 0; j < keypoints.length; j++) {
            const x = keypoints[j][0] * widthRatio;
            const y = keypoints[j][1] * heightRatio;
            const bodyPart = Object.keys(colors)[j];
            ctx.beginPath();
            ctx.arc(x, y, 2.5, 0, 2 * Math.PI);
            ctx.fillStyle = colors[bodyPart];
            ctx.fill();
            ctx.closePath();
          }
        }
      }

      canvas.addEventListener('webglcontextlost', (e) => {
        e.preventDefault();
        console.warn("WebGL context lost. Consider reloading the page.");
      });

      // Run inference
      window.addEventListener("load", () => {
        const video = document.querySelector("#video");
        const canvas = document.querySelector("canvas");
        const context = canvas.getContext("2d");
        // video.style.display = "block";

        // Set video stream constraints
        const constraints = {
          audio: false,
          video: {
            // width: { ideal: 9999 },
            // height: { ideal: 9999 },
            width: 1920,
            height: 1080,                     // the exact resolution of a webcam
            facingMode: "environment",
            exposureMode: "continuous",       // tries to keep auto exposure
            whiteBalanceMode: "continuous",   // auto white balance
            focusMode: "continuous"
          },
        };

        // Request access to the user"s camera
        navigator.mediaDevices
          .getUserMedia(constraints)
          .then((stream) => {
            const settings = stream.getVideoTracks()[0].getSettings();
            console.log("Video resolution:", settings.width + "x" + settings.height);

            video.srcObject = stream;
            video.play();

            video.onloadedmetadata = () => {
              setInterval(() => {
                detectVideo(video, model(), canvas);
              }, 100);
            };
          })
          .catch((err) => {
            console.error(err);
          });

      })
    </script>
</body>

</html>